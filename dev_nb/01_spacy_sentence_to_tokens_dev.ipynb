{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Tokenize Sentences\n",
    "\n",
    "## But first, why tokens?\n",
    "Short answer: API vendors charge based on usage, by the token (kilo-Token or mega-Token)\n",
    "\n",
    "Long answer: Language models like BERT and GPT count and process text using tokens rather than words for a few key reasons:\n",
    "1. Words don't capture morphology - Tokens allow accounting for prefixes, suffixes, and word forms that expand the vocabulary. This improves generalization.\n",
    "1. Non-word tokens are useful - Tokens can include punctuation, special symbols, numbers, etc. which carry meaning.\n",
    "1. Subword tokenization - Splitting rare words into subword units via tokenization like WordPiece improves vocabulary coverage.\n",
    "1. Vocabulary size limits - Models have a predefined vocabulary size based on memory constraints. Tokens maximize what can fit.\n",
    "\n",
    "## spaCy cheatsheet\n",
    "https://www.datacamp.com/cheat-sheet/spacy-cheat-sheet-advanced-nlp-in-python\n",
    "\n",
    "## How to tokenize sentences\n",
    "First, install spaCy, and then the english language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# https://spacy.io/api/top-level#spacy.prefer_gpu\n",
    "# Allocate data and perform operations on GPU, if available. \n",
    "# If data has already been allocated on CPU, it will not be moved. \n",
    "# Ideally, this function should be called right after importing spaCy and before loading any pipelines.\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "# pprint.pprint(spacy.info())\n",
    "print(json.dumps(spacy.info(), indent=4))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\n",
    "    \"This is the first sentence that you will tokenize.\"\n",
    ")\n",
    "type(doc)\n",
    "[token.text for token in doc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "```\n",
    "[\n",
    "'This',\n",
    "'is',\n",
    "'the',\n",
    "'first',\n",
    "'sentence',\n",
    "'that',\n",
    "'you',\n",
    "'will',\n",
    "'tokenize',\n",
    "'.'\n",
    "]\n",
    "```    \n",
    "   9 words, but 1 doc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\n",
    "    \"This shouldn't have been the first sentence that you will tokenize. Better if wasn't third, also.\"\n",
    ")\n",
    "type(doc)\n",
    "print('{} doc'.format(len(doc)))\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 words and 21 tokens\n",
    "\n",
    "```\n",
    "[\n",
    " 'This',\n",
    " 'should',\n",
    " \"n't\",\n",
    " 'have',\n",
    " 'been',\n",
    " 'the',\n",
    " 'first',\n",
    " 'sentence',\n",
    " 'that',\n",
    " 'you',\n",
    " 'will',\n",
    " 'tokenize',\n",
    " '.',\n",
    " 'Better',\n",
    " 'if',\n",
    " 'was',\n",
    " \"n't\",\n",
    " 'third',\n",
    " ',',\n",
    " 'also',\n",
    " '.'\n",
    " ]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "file_name = \"wiki_spacy.txt\"\n",
    "doc = nlp(pathlib.Path(file_name).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "print ([token.text for token in doc])\n",
    "\n",
    "# sentences\n",
    "sentences = list(doc.sents)\n",
    "len(sentences)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(f\"{sentence[:5]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# language detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import spacy\n",
    "# Ideally, this function should be called right after importing spaCy and before loading any pipelines.\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "@Language.factory(\"language_detector\")\n",
    "def get_lang_detector(nlp, name):\n",
    "   return LanguageDetector()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')  # 1\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp.add_pipe('language_detector', last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_language = \"Er lebt mit seinen Eltern und seiner Schwester in Berlin.\"\n",
    "doc = nlp(unknown_language)\n",
    "detect_language = doc._.language \n",
    "print(detect_language)\n",
    "\n",
    "unknown_language = '왜 내 주변에는 맛있는 한식당이 없지?'\n",
    "doc = nlp(unknown_language)\n",
    "detect_language = doc._.language \n",
    "print(detect_language)\n",
    "\n",
    "# sentences woo hoo!\n",
    "print ([token.text for token in doc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
