{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning & Formatting Data\n",
    "\n",
    "Data was harvested, cleaned and formatted from \n",
    "https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered\n",
    "License: apache-2.0\n",
    "\n",
    "AFAIK, the raw dataset was harvested from HTML chats in mulitple languages.\n",
    "We want clean, English. English because we will use this data for semantic drift detection and I personally can only judge whether English has drifted.\n",
    "\n",
    "It's great data but not perfect for our needs\n",
    "1. Prompt tokens may be too big for model\n",
    "2. Completion tokens may be too big for model\n",
    "3. May be non-English characters/text\n",
    "4. Weird characters, weird line endings\n",
    "4. Lots of spaces and newline where not needed.\n",
    "5. We can add token and sentence counts while we're cleaning to make the clean data more useful.\n",
    "6. Replace id with uuid4.\n",
    "\n",
    "RAW:\n",
    "```JSON\n",
    "{\n",
    "    \"prompt\": \"Answer as if you were the medical director of an anesthesia service. Develop a preoperatively testing algorithm reported in a chart format. Select specific tests that are required for each test based on both particular medical diagnoses as well as by the type of operation is planned.\",\n",
    "    \"completion\": \"Preoperative Testing Algorithm  Patient's Medical Diagnosis:   Type of Operation:   Recommended Testing:  | Test | Medical Diagnosis | Type of Operation | | --- | --- | --- | | Complete Blood Count (CBC) | All | All | | Coagulation Panel | Bleeding disorders, liver disease | All major surgeries, neurosurgery, orthopedic surgeries | | Electrol\",\n",
    "    \"id\": \"RzBNjvK\"\n",
    "  }\n",
    "  ```\n",
    "CLEAN:\n",
    "```JSON\n",
    "{\n",
    "    \"prompt_token_len\": 50,\n",
    "    \"prompt_sent_len\": 3,\n",
    "    \"prompt\": \"Answer as if you were the medical director of an anesthesia service. Develop a preoperatively testing algorithm reported in a chart format. Select specific tests that are required for each test based on both particular medical diagnoses as well as by the type of operation is planned.\",\n",
    "    \"completion_token_len\": 48,\n",
    "    \"completion_sent_len\": 2,\n",
    "    \"completion\": \"Preoperative Testing Algorithm Patient's Medical Diagnosis: Type of Operation: Recommended Testing: Test Medical Diagnosis Type of Operation --- --- --- Complete Blood Count (CBC) All All Coagulation Panel Bleeding disorders, liver disease All major surgeries, neurosurgery, orthopedic surgeries Electrol\",\n",
    "    \"chat_id\": \"14789613-f5af-4385-9d80-3058fbdb1f8c\"\n",
    "}\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def print_runtime():\n",
    "    run_time = datetime.datetime.now().replace(second=0, microsecond=0)\n",
    "    print(\"Last run time: {}\".format(run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# Ideally, this function should be called right after importing spaCy and before loading any pipelines.\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "# pprint.pprint(spacy.info())\n",
    "print(json.dumps(spacy.info(), indent=4))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print_runtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import re\n",
    "\n",
    "print(\"Creating language_detector\")\n",
    "@Language.factory(\"language_detector\")\n",
    "def get_lang_detector(nlp, name):\n",
    "   return LanguageDetector()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')  # 1\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp.add_pipe('language_detector', last=True)\n",
    "\n",
    "print(\"language_detector created\")\n",
    "print_runtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import raw json data\n",
    "\n",
    "print(\"Loading rawJsonFile\")\n",
    "\n",
    "# start small\n",
    "srcRawJsonFilePath = '..\\\\data\\\\raw\\\\ShareGptChatPairs_dev_32.json'\n",
    "destCleanJsonFilePath = '..\\\\data\\\\clean\\\\ShareGptChatPairs_dev_32_cleaned_formatted.json'\n",
    "\n",
    "# then do the big file    \n",
    "# srcRawJsonFilePath = '..\\\\data\\\\raw\\\\'..\\\\data\\\\raw\\\\ShareGptChatPairs_3330.json'\n",
    "# destCleanJsonFilePath = '..\\\\data\\\\clean\\\\ShareGptChatPairs_3330_cleaned_formatted.json'\n",
    "\n",
    "\n",
    "rawJsonFile = open(srcRawJsonFilePath)\n",
    "\n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "rawData = json.load(rawJsonFile)\n",
    "# Closing file\n",
    "rawJsonFile.close()\n",
    "\n",
    "print(\"rawJsonFile loaded\")\n",
    "\n",
    "def is_english(unknown_language):\n",
    "    doc = nlp(unknown_language)\n",
    "    detected_lang = doc._.language \n",
    "    # print(detected_lang)\n",
    "    return detected_lang['language'] == 'en' and detected_lang['score'] > 0.9\n",
    "\n",
    "def is_only_english(json):\n",
    "    pok = is_english(json[\"prompt\"])\n",
    "    cok = is_english(json[\"completion\"])\n",
    "    return pok and cok\n",
    "\n",
    "def clean_string(string):\n",
    "    #string = text.encode(\"ascii\", \"replace\")\n",
    "    return \" \".join((re.sub('\"', '', string)).replace(\"\\\\\", \"\").replace(\"_\", \"\").replace(\"|\", \"\").split())\n",
    "\n",
    "def gen_uuid():\n",
    "    return clean_string(json.dumps(uuid.uuid4(), default=str))\n",
    "\n",
    "def clean_data(raw):\n",
    "    clean_prompt = clean_string(raw[\"prompt\"])\n",
    "    prompt_doc = nlp(clean_prompt)\n",
    "    prompt_tok_len = len(prompt_doc)\n",
    "    prompt_sents = list(prompt_doc.sents)\n",
    "    prompt_sent_count =len(prompt_sents)\n",
    "                       \n",
    "    clean_completion = clean_string(raw[\"completion\"])\n",
    "    completion_doc = nlp(clean_completion)\n",
    "    completion_tok_len = len(completion_doc)\n",
    "    completion_sents = list(completion_doc.sents)\n",
    "    completion_sent_count =len(completion_sents)\n",
    "    return {\n",
    "        \"prompt_token_len\": prompt_tok_len,\n",
    "        \"prompt_sent_len\": prompt_sent_count,\n",
    "        \"prompt\": clean_prompt,\n",
    "        \"completion_token_len\": completion_tok_len,\n",
    "        \"completion_sent_len\": completion_sent_count,\n",
    "        \"completion\": clean_completion,\n",
    "        \"chat_id\": gen_uuid()\n",
    "    }  \n",
    "\n",
    "print_runtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how long will it take to clean your data?\n",
    "Have a look at this discussion about what should be a straightforward problem:\n",
    "\n",
    "https://stackoverflow.com/questions/7370801/how-do-i-measure-elapsed-time-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "started = timer()\n",
    "cleaned = []\n",
    "count = 0\n",
    "max = 100\n",
    "\n",
    "for i, d in enumerate(rawData):\n",
    "    # we only accept eng lang\n",
    "    if is_only_english(d):\n",
    "        cleaned.append(clean_data(d))\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count % 10 == 0:\n",
    "        print(count)\n",
    "    \n",
    "    # limit the work\n",
    "    if(count>max):\n",
    "        break\n",
    "    \n",
    "completed = timer()\n",
    "print(\"duration for {} records was {} seconds.\".format(count, completed - started))\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "#print(json.dumps(cleaned[0], indent=4))\n",
    "print_runtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "duration for 100 records was 29.424200800000108  seconds.\n",
    "\n",
    "On my mchaine, spacy.prefer_gpu() makes no difference\n",
    "\n",
    "Next...\n",
    "\n",
    "ShareGptChatPairs_3330_cleaned.json will be about 2415 cleaned and formatted chat pairs.\n",
    "Writing the cleaned data to file will be quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write cleaned json to the cleaned directory\n",
    "with open(destCleanJsonFilePath, 'w') as f:\n",
    "    f.write(\"[\\n\")\n",
    "    idx = 0\n",
    "      \n",
    "    count = len(cleaned)\n",
    "    while idx < count:\n",
    "        f.write(json.dumps(cleaned[idx], indent=4))\n",
    "        if idx < count-1:\n",
    "            f.write(\",\\n\") \n",
    "        idx += 1\n",
    "\n",
    "        # prevent writing to file in an endless loop in the vent of bad code.       \n",
    "        if(idx > max):\n",
    "            break \n",
    "              \n",
    "    f.write(\"\\n]\")\n",
    "    \n",
    "    print(\"Finished writing to file\")\n",
    "\n",
    "    print_runtime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
